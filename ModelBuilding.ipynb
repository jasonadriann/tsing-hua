{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\jason\\semester 8\\Magang\n",
      "c:\\jason\\semester 8\\Magang\\Tsing-hua/low-resolution\n",
      "c:\\jason\\semester 8\\Magang\\Tsing-hua/TrainAndValList\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "print(ROOT_DIR)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'Tsing-hua/low-resolution')\n",
    "print(DATA_DIR)\n",
    "TRAINVAL_DIR = os.path.join(ROOT_DIR, 'Tsing-hua/TrainAndValList')\n",
    "print(TRAINVAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_path = os.path.join(TRAINVAL_DIR, 'train.lst')\n",
    "\n",
    "with open(train_list_path, \"r\") as f:\n",
    "    lst_contents = f.readlines()\n",
    "\n",
    "train_array_of_paths = [line.strip() for line in lst_contents]\n",
    "\n",
    "train_file_paths = []\n",
    "\n",
    "for img_path in train_array_of_paths:\n",
    "    joined_path = DATA_DIR + img_path[2:]\n",
    "    train_file_paths.append(joined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_path = os.path.join(TRAINVAL_DIR, 'validation.lst')\n",
    "\n",
    "with open(val_list_path, \"r\") as f:\n",
    "    lst_contents = f.readlines()\n",
    "\n",
    "val_array_of_paths = [line.strip() for line in lst_contents]\n",
    "\n",
    "val_file_paths = []\n",
    "\n",
    "for img_path in val_array_of_paths:\n",
    "    joined_path = DATA_DIR + img_path[2:]\n",
    "    val_file_paths.append(joined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "RGB_MEAN = [0.51442681, 0.43435301, 0.33421855]\n",
    "RGB_STD = [0.24099932, 0.246478, 0.23652802]\n",
    "INPUT_SIZE = (384, 384)\n",
    "\n",
    "TRAIN_TRANSFORMATION = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.CenterCrop(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=RGB_MEAN, std=RGB_STD)\n",
    "])\n",
    "\n",
    "VAL_TRANSFORMATION = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=RGB_MEAN, std=RGB_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    \"\"\"Strutured Image Folder Dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        Path to image root directory\n",
    "    transform : Optional[torchvision.transforms.Compose], optional\n",
    "        Data augmentation pipeline, by default None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, filepath_list: Optional[list] = None, transform: Optional[torchvision.transforms.Compose] = None):\n",
    "        self.root_dir = root_dir\n",
    "        if not os.path.exists(root_dir):\n",
    "            raise RuntimeError(f'Path to dataset is not valid')\n",
    "        self.list_images =  filepath_list\n",
    "        self.transform = transform\n",
    "        self.labels_name = os.listdir(self.root_dir)\n",
    "        self.labels_name.sort()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get number of images.\"\"\"\n",
    "        return len(self.list_images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get sample for the current idx.\"\"\"\n",
    "        img_path = self.list_images[idx]\n",
    "\n",
    "        # NOTE: cv2 read image as BGR.\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert class name to label 0-10\n",
    "        # Note : root_path/class_name/image_file.jpg\n",
    "        class_name = img_path.split('/')[-2]\n",
    "        label_index = self.labels_name.index(class_name)\n",
    "        label_index = [label_index]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # If no transformations, convert to C,H,W\n",
    "        # Normalize to [0,1]\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            image = torch.from_numpy()\n",
    "            image /= 255.0\n",
    "\n",
    "        return image, torch.tensor(label_index, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolderDataset(root_dir=DATA_DIR, filepath_list= train_file_paths, transform=TRAIN_TRANSFORMATION)\n",
    "image, label = train_dataset[10]\n",
    "\n",
    "assert len(train_dataset) == 65228\n",
    "assert isinstance(image, torch.Tensor)\n",
    "assert isinstance(label, torch.Tensor)\n",
    "\n",
    "val_dataset = ImageFolderDataset(root_dir=DATA_DIR, filepath_list= val_file_paths, transform=VAL_TRANSFORMATION)\n",
    "image, label = val_dataset[10]\n",
    "\n",
    "\n",
    "print(len(train_dataset.labels_name))\n",
    "\n",
    "\n",
    "# val_dataset = ImageFolderDataset(root_dir=VAL_DATA_PATH, transform=VAL_TRANSFORMATION)\n",
    "# image, label = train_dataset[10]\n",
    "\n",
    "assert len(val_dataset) == 5200\n",
    "assert isinstance(image, torch.Tensor)\n",
    "assert isinstance(label, torch.Tensor)\n",
    "\n",
    "# Cleanup to reduce memory\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "image, label = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "class TsingHuaLitDatamodule(LightningDataModule):\n",
    "    \"\"\"LightningDataModule for Food101 Data Pipeline.\n",
    "\n",
    "    Read the docs:\n",
    "        https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str, optional\n",
    "        FiftyOne dataset directory, by default 'data/'\n",
    "    input_size : List[int], optional\n",
    "        Input model size, by default [600, 500]\n",
    "    batch_size : int, optional\n",
    "        Number of training batch size, by default 64\n",
    "    num_workers : int, optional\n",
    "        Number of worksers to process data, by default 0\n",
    "    pin_memory : bool, optional\n",
    "        Enable memory pinning, by default False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = DATA_DIR,\n",
    "        input_size: Tuple[int, int] = (600, 500),\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Get number of classes.\"\"\"\n",
    "        return 130\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load the data with specified stage.\"\"\"\n",
    "        if stage in ['train', 'fit', None] and self.data_train is None:\n",
    "            self.data_train = ImageFolderDataset(\n",
    "                root_dir=self.hparams.data_dir, filepath_list=train_file_paths, transform=TRAIN_TRANSFORMATION)\n",
    "            if len(self.data_train) == 0:\n",
    "                raise ValueError('Train dataset is empty.')\n",
    "        if stage in ['validation', 'test', 'fit', None]:\n",
    "            if self.data_val is None:\n",
    "                self.data_val = ImageFolderDataset(\n",
    "                    root_dir=self.hparams.data_dir, filepath_list=val_file_paths, transform=VAL_TRANSFORMATION)\n",
    "                if len(self.data_val) == 0:\n",
    "                    raise ValueError('Validation dataset is empty.')\n",
    "            if self.data_test is None:\n",
    "                self.data_test = ImageFolderDataset(\n",
    "                    root_dir=self.hparams.data_dir, filepath_list=val_file_paths, transform=VAL_TRANSFORMATION)\n",
    "                if len(self.data_test) == 0:\n",
    "                    raise ValueError('Test dataset is empty.')\n",
    "        if stage == 'predict':\n",
    "            if self.data_test is None:\n",
    "                self.data_predict = ImageFolderDataset(\n",
    "                    root_dir=self.hparams.data_dir, transform=VAL_TRANSFORMATION)\n",
    "                if len(self.data_predict) == 0:\n",
    "                    raise ValueError('Predict dataset is empty.')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Get train dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Get validation dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Get test dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TsingHuaLitDatamodule(data_dir=DATA_DIR, batch_size=4)\n",
    "\n",
    "assert not dm.data_train and not dm.data_val and not dm.data_test\n",
    "\n",
    "dm.setup()\n",
    "assert dm.data_train and dm.data_val and dm.data_test\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()\n",
    "test_dataloader = dm.test_dataloader()\n",
    "assert train_dataloader and val_dataloader and test_dataloader\n",
    "\n",
    "# train = 65228, val = 1300, test = 1300\n",
    "assert len(dm.data_train) + len(dm.data_val) + len(dm.data_test) == 75628\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "x, y = batch\n",
    "assert len(x) == 4\n",
    "assert len(y) == 4\n",
    "assert x.dtype == torch.float32\n",
    "assert y.dtype == torch.int64\n",
    "\n",
    "# # Cleanup\n",
    "dm = None\n",
    "batch = None\n",
    "train_dataloader, val_dataloader, test_dataloader = None, None, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
